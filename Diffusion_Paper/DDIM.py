import torch
from diffusers import StableDiffusionInstructPix2PixPipeline, DDIMScheduler
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

def setup_model():
    """
    ì‚¬ì „ í•™ìŠµëœ InstructPix2Pix ëª¨ë¸ ë¡œë“œ (DDIM ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©)
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')
    
    model_id = "timbrooks/instruct-pix2pix"
    
    print("ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
        model_id, 
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        safety_checker=None,
        use_safetensors=True
    )
    
    pipe = pipe.to(device)
    
    # DDIM ìŠ¤ì¼€ì¤„ëŸ¬
    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
    
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (DDIM ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©)")
    return pipe, device

def age_transform_single(image_path, pipe, device):
    """
    í•˜ë‚˜ì˜ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ëŠ™ê²Œ ë³€í™˜í•©ë‹ˆë‹¤
    
    Args:
        image_path: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
        pipe: Diffusion íŒŒì´í”„ë¼ì¸
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
    
    Returns:
        original: ì›ë³¸ ì´ë¯¸ì§€
        aged: ëŠ™ì€ ë²„ì „ ì´ë¯¸ì§€
    """
    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
    image = Image.open(image_path).convert("RGB")
    
    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (512x512ê°€ ìµœì )
    image = image.resize((512, 512))
    
    # ë…¸í™” í”„ë¡¬í”„íŠ¸
    prompt = "make the person look like an elderly grandfather, aged 70-80 years old, with wrinkles and gray hair"
    
    print(f"\n'{image_path}' ë³€í™˜ ì¤‘...")
    print(f"í”„ë¡¬í”„íŠ¸: {prompt}")
    
    # ì´ë¯¸ì§€ ë³€í™˜
    aged = pipe(
        prompt=prompt,
        image=image,
        num_inference_steps=50,
        image_guidance_scale=1.5,
        guidance_scale=7.5,
    ).images[0]
    
    return image, aged

def process_two_images(image_path1, image_path2, pipe, device):
    """
    ë‘ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê°ê° ëŠ™ê²Œ ë³€í™˜í•©ë‹ˆë‹¤
    
    Args:
        image_path1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê²½ë¡œ
        image_path2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ ê²½ë¡œ
        pipe: Diffusion íŒŒì´í”„ë¼ì¸
        device: ë””ë°”ì´ìŠ¤
    """
    print("\n" + "="*60)
    print("ë‘ ê°œ ì‚¬ì§„ ë…¸í™” ë³€í™˜ ì‹œì‘")
    print("="*60)
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ë³€í™˜
    print("\n[1/2] ì²« ë²ˆì§¸ ì‚¬ì§„ ì²˜ë¦¬ ì¤‘...")
    original1, aged1 = age_transform_single(image_path1, pipe, device)
    
    # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ ë³€í™˜
    print("\n[2/2] ë‘ ë²ˆì§¸ ì‚¬ì§„ ì²˜ë¦¬ ì¤‘...")
    original2, aged2 = age_transform_single(image_path2, pipe, device)
    
    # ê²°ê³¼ ì‹œê°í™”
    visualize_two_results(original1, aged1, original2, aged2, 
                          image_path1, image_path2)
    
    return (original1, aged1), (original2, aged2)

def visualize_two_results(original1, aged1, original2, aged2, 
                          path1, path2):
    """
    ë‘ ê°œì˜ ì›ë³¸ê³¼ ë³€í™˜ëœ ì´ë¯¸ì§€ë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤
    """
    plt.figure(figsize=(16, 8))
    
    # ì²« ë²ˆì§¸ ì‚¬ì§„ ì„¸íŠ¸
    plt.subplot(2, 2, 1)
    plt.imshow(original1)
    plt.title(f"Original 1\n({path1.split('/')[-1]})", 
              fontsize=12, fontweight='bold')
    plt.axis('off')
    
    plt.subplot(2, 2, 2)
    plt.imshow(aged1)
    plt.title("Aged Version 1\n(70-80 years old)", 
              fontsize=12, fontweight='bold', color='darkred')
    plt.axis('off')
    
    # ë‘ ë²ˆì§¸ ì‚¬ì§„ ì„¸íŠ¸
    plt.subplot(2, 2, 3)
    plt.imshow(original2)
    plt.title(f"Original 2\n({path2.split('/')[-1]})", 
              fontsize=12, fontweight='bold')
    plt.axis('off')
    
    plt.subplot(2, 2, 4)
    plt.imshow(aged2)
    plt.title("Aged Version 2\n(70-80 years old)", 
              fontsize=12, fontweight='bold', color='darkred')
    plt.axis('off')
    
    plt.tight_layout()
    plt.savefig('two_faces_aging_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"\nâœ… ê²°ê³¼ê°€ 'two_faces_aging_results.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def save_individual_results(original1, aged1, original2, aged2):
    """
    ê° ê²°ê³¼ë¥¼ ê°œë³„ íŒŒì¼ë¡œë„ ì €ì¥í•©ë‹ˆë‹¤
    """
    aged1.save('aged_photo1.png')
    aged2.save('aged_photo2.png')
    
    print(f"âœ… ê°œë³„ íŒŒì¼ë„ ì €ì¥ ì™„ë£Œ:")
    print(f"   - aged_photo1.png")
    print(f"   - aged_photo2.png")

if __name__ == '__main__':
    # ëª¨ë¸ ì„¤ì •
    pipe, device = setup_model()
    
    print("\n" + "="*60)
    print("ì–¼êµ´ ë‚˜ì´ ë³€í™˜ Diffusion ëª¨ë¸ (2ì¥ ë™ì‹œ ì²˜ë¦¬)")
    print("="*60)
    
    # ğŸ”¥ ì—¬ê¸°ì— ë‘ ê°œì˜ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ğŸ”¥
    image_path1 = "./refs/test/test3.jpg"  # <- ì²« ë²ˆì§¸ ì‚¬ì§„ ê²½ë¡œ
    image_path2 = "./refs/test/test4.jpg"  # <- ë‘ ë²ˆì§¸ ì‚¬ì§„ ê²½ë¡œ
    
    # ë‘ ì´ë¯¸ì§€ ì²˜ë¦¬
    (orig1, aged1), (orig2, aged2) = process_two_images(
        image_path1, image_path2, pipe, device
    )
    
    # ê°œë³„ íŒŒì¼ë¡œë„ ì €ì¥
    save_individual_results(orig1, aged1, orig2, aged2)
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)
    print(f"CUDA ì‚¬ìš©: {torch.cuda.is_available()}")
    print(f"ìŠ¤ì¼€ì¤„ëŸ¬: DDIM (Deterministic Sampling)")
    print("\nğŸ’¡ TIP:")
    print("- ë” ê°•í•œ ë…¸í™” íš¨ê³¼: guidance_scale ê°’ì„ ë†’ì´ì„¸ìš” (7.5 â†’ 10)")
    print("- ë” ë†’ì€ í’ˆì§ˆ: num_inference_stepsë¥¼ ëŠ˜ë¦¬ì„¸ìš” (50 â†’ 100)")
    print("- ì›ë³¸ ìœ ì‚¬ë„ ì¡°ì ˆ: image_guidance_scale ì¡°ì • (1.5 â†’ 1.0~2.0)")